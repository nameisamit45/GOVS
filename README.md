
# Gesture Operated Vehicle System (GOVS) ğŸš—ğŸ–ï¸

**GOVS** (Gesture Operated Vehicle System), also known as **HandiCar**, is a real-time, gesture-controlled robotic vehicle designed for intuitive hands-free navigation. It uses motion sensors and wireless communication to allow usersâ€”especially individuals with physical impairmentsâ€”to control a car with hand gestures.

## ğŸ§  Motivation

Traditional control methods like joysticks or smartphone apps are not universally accessible. GOVS aims to overcome these limitations by offering a **gesture-based**, **low-cost**, and **real-time** control system that is:
- Easy to use
- Responsive
- Assistive in nature
- Expandable to smart mobility and automation applications

---

## ğŸš€ Features

- **MPU6050-based gesture control**
- **Bidirectional RF communication** using nRF24L01+
- **Obstacle detection and rerouting** using HC-SR04 ultrasonic sensor + servo scanning
- **Low power mode** for idle states
- **OLED status display** on controller side
- **Emergency stop system**
- **Multidirectional Mecanum wheel chassis**
- **Visual feedback using RGB LEDs**
- **Expandable to voice control and mobile app integration**

---

## ğŸ“¦ System Architecture

```plaintext
[MPU6050 Sensor] 
     â†“
[Arduino Controller] 
     â†“
[nRF24L01 Transmitter] 
     â†”
[nRF24L01 Receiver] 
     â†“
[Arduino Car] â†’ [Motors, LED Indicators, Sensors]
```

---

## ğŸ§© Subsystems

- **Gesture Input Subsystem:** Maps pitch and roll angles to x/y axis control values.
- **Communication Subsystem:** RF-based data transfer with ACK and feedback.
- **Motion Subsystem:** Mecanum wheels allow forward, backward, rotation, and lateral motion.
- **Obstacle Detection:** Servo-mounted ultrasonic sensor with dual-phase avoidance + emergency stop.
- **User Feedback:** OLED on controller + RGB LEDs on vehicle.
- **Power Management:** Low power and emergency modes.

---

## ğŸ“‚ Repository Structure

```
GOVS/
â”œâ”€â”€ codes/
â”‚   â”œâ”€â”€ transmitter.ino
â”‚   â””â”€â”€ receiver.ino
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ECE49200 - Final Report.pdf
â”‚   â””â”€â”€ Poster.pdf
â”œâ”€â”€ images/
â”‚   â””â”€â”€ system_diagram.png
â””â”€â”€ README.md
```

---

## âš™ï¸ Technologies Used

- **Microcontrollers:** Arduino Uno & Nano
- **Wireless Module:** NRF24L01+
- **Motion Sensor:** MPU6050
- **Ultrasonic Sensor:** HC-SR04
- **Motor Driver:** L298N
- **Display:** SSD1306 OLED
- **Programming Language:** C++
- **IDE:** Arduino IDE


---

## ğŸ“ˆ Future Improvements

- AI-based gesture classification using ML
- Voice command fallback mode
- Mobile app integration (via BLE)
- Enhanced obstacle mapping using multiple sensors
- Cloud-based data logging and analytics

---

## ğŸ‘¨â€ğŸ’» Authors

This project was developed by the **HandiCar Team** as part of ECE49200 at Purdue University:

- Amit Vardhan Suryadevara
- Biak Par
- Divya Patel
- Maha Ali
- Martine Cardichon
- Yamini Sri Krubha

Project Supervisor: **Prof. Greg Sturm**

---

## ğŸ“„ License

This project is for educational and research purposes. For inquiries about reuse or contributions, please contact the project authors.

---

## ğŸŒ Standards Compliance

This system adheres to:
- **ISO/IEC 30113-1:2025** (Gesture-based interface)
- **ISO/IEC 25010** (System & Software Quality Model)

---

## ğŸ“· Screenshots

![System Architecture](images/system_diagram.png)
*(Add your own diagrams or screenshots here)*

---

## ğŸ§  ABET Learning Outcomes

This capstone project fulfills multiple ABET outcomes including:
- Problem solving
- Engineering design
- Team collaboration
- Communication
- Ethics & constraints awareness
